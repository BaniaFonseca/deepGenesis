1.13.1
1.13.1
Saved model to disk
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 256, 256, 3)  0                                            
__________________________________________________________________________________________________
conv_1 (Conv2D)                 (None, 125, 125, 64) 9408        input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 125, 125, 64) 256         conv_1[0][0]                     
__________________________________________________________________________________________________
leaky_re_lu_1 (LeakyReLU)       (None, 125, 125, 64) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 62, 62, 64)   0           leaky_re_lu_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 62, 62, 64)   256         max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)       (None, 62, 62, 64)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv_31 (Conv2D)                (None, 62, 62, 64)   4096        leaky_re_lu_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 62, 62, 64)   256         conv_31[0][0]                    
__________________________________________________________________________________________________
leaky_re_lu_3 (LeakyReLU)       (None, 62, 62, 64)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 62, 62, 64)   36864       leaky_re_lu_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 62, 62, 64)   256         conv2d_1[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 62, 62, 64)   0           batch_normalization_4[0][0]      
                                                                 leaky_re_lu_2[0][0]              
__________________________________________________________________________________________________
leaky_re_lu_4 (LeakyReLU)       (None, 62, 62, 64)   0           add_1[0][0]                      
__________________________________________________________________________________________________
conv_32 (Conv2D)                (None, 62, 62, 64)   4096        leaky_re_lu_4[0][0]              
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 62, 62, 64)   256         conv_32[0][0]                    
__________________________________________________________________________________________________
leaky_re_lu_5 (LeakyReLU)       (None, 62, 62, 64)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 62, 62, 64)   36864       leaky_re_lu_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 62, 62, 64)   256         conv2d_2[0][0]                   
__________________________________________________________________________________________________
add_2 (Add)                     (None, 62, 62, 64)   0           batch_normalization_6[0][0]      
                                                                 leaky_re_lu_4[0][0]              
__________________________________________________________________________________________________
leaky_re_lu_6 (LeakyReLU)       (None, 62, 62, 64)   0           add_2[0][0]                      
__________________________________________________________________________________________________
conv_33 (Conv2D)                (None, 62, 62, 64)   4096        leaky_re_lu_6[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 62, 62, 64)   256         conv_33[0][0]                    
__________________________________________________________________________________________________
leaky_re_lu_7 (LeakyReLU)       (None, 62, 62, 64)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 62, 62, 64)   36864       leaky_re_lu_7[0][0]              
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 62, 62, 64)   256         conv2d_3[0][0]                   
__________________________________________________________________________________________________
add_3 (Add)                     (None, 62, 62, 64)   0           batch_normalization_8[0][0]      
                                                                 leaky_re_lu_6[0][0]              
__________________________________________________________________________________________________
leaky_re_lu_8 (LeakyReLU)       (None, 62, 62, 64)   0           add_3[0][0]                      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 30, 30, 128)  73728       leaky_re_lu_8[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 30, 30, 128)  512         conv2d_4[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_9 (LeakyReLU)       (None, 30, 30, 128)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 30, 30, 128)  16384       leaky_re_lu_9[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 30, 30, 128)  512         conv2d_5[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_10 (LeakyReLU)      (None, 30, 30, 128)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 30, 30, 128)  147456      leaky_re_lu_10[0][0]             
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 30, 30, 128)  512         conv2d_6[0][0]                   
__________________________________________________________________________________________________
add_4 (Add)                     (None, 30, 30, 128)  0           batch_normalization_11[0][0]     
                                                                 leaky_re_lu_9[0][0]              
__________________________________________________________________________________________________
leaky_re_lu_11 (LeakyReLU)      (None, 30, 30, 128)  0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 30, 30, 128)  16384       leaky_re_lu_11[0][0]             
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 30, 30, 128)  512         conv2d_7[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_12 (LeakyReLU)      (None, 30, 30, 128)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 30, 30, 128)  147456      leaky_re_lu_12[0][0]             
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 30, 30, 128)  512         conv2d_8[0][0]                   
__________________________________________________________________________________________________
add_5 (Add)                     (None, 30, 30, 128)  0           batch_normalization_13[0][0]     
                                                                 leaky_re_lu_11[0][0]             
__________________________________________________________________________________________________
leaky_re_lu_13 (LeakyReLU)      (None, 30, 30, 128)  0           add_5[0][0]                      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 30, 30, 128)  16384       leaky_re_lu_13[0][0]             
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 30, 30, 128)  512         conv2d_9[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_14 (LeakyReLU)      (None, 30, 30, 128)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 30, 30, 128)  147456      leaky_re_lu_14[0][0]             
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 30, 30, 128)  512         conv2d_10[0][0]                  
__________________________________________________________________________________________________
add_6 (Add)                     (None, 30, 30, 128)  0           batch_normalization_15[0][0]     
                                                                 leaky_re_lu_13[0][0]             
__________________________________________________________________________________________________
leaky_re_lu_15 (LeakyReLU)      (None, 30, 30, 128)  0           add_6[0][0]                      
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 30, 30, 128)  16384       leaky_re_lu_15[0][0]             
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 30, 30, 128)  512         conv2d_11[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_16 (LeakyReLU)      (None, 30, 30, 128)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 30, 30, 128)  147456      leaky_re_lu_16[0][0]             
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 30, 30, 128)  512         conv2d_12[0][0]                  
__________________________________________________________________________________________________
add_7 (Add)                     (None, 30, 30, 128)  0           batch_normalization_17[0][0]     
                                                                 leaky_re_lu_15[0][0]             
__________________________________________________________________________________________________
leaky_re_lu_17 (LeakyReLU)      (None, 30, 30, 128)  0           add_7[0][0]                      
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 14, 14, 256)  294912      leaky_re_lu_17[0][0]             
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 14, 14, 256)  1024        conv2d_13[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_18 (LeakyReLU)      (None, 14, 14, 256)  0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 14, 14, 256)  65536       leaky_re_lu_18[0][0]             
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 14, 14, 256)  1024        conv2d_14[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_19 (LeakyReLU)      (None, 14, 14, 256)  0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 14, 14, 256)  589824      leaky_re_lu_19[0][0]             
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 14, 14, 256)  1024        conv2d_15[0][0]                  
__________________________________________________________________________________________________
add_8 (Add)                     (None, 14, 14, 256)  0           batch_normalization_20[0][0]     
                                                                 leaky_re_lu_18[0][0]             
__________________________________________________________________________________________________
leaky_re_lu_20 (LeakyReLU)      (None, 14, 14, 256)  0           add_8[0][0]                      
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 14, 14, 256)  65536       leaky_re_lu_20[0][0]             
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 14, 14, 256)  1024        conv2d_16[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_21 (LeakyReLU)      (None, 14, 14, 256)  0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 14, 14, 256)  589824      leaky_re_lu_21[0][0]             
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 14, 14, 256)  1024        conv2d_17[0][0]                  
__________________________________________________________________________________________________
add_9 (Add)                     (None, 14, 14, 256)  0           batch_normalization_22[0][0]     
                                                                 leaky_re_lu_20[0][0]             
__________________________________________________________________________________________________
leaky_re_lu_22 (LeakyReLU)      (None, 14, 14, 256)  0           add_9[0][0]                      
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 14, 14, 256)  65536       leaky_re_lu_22[0][0]             
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 14, 14, 256)  1024        conv2d_18[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_23 (LeakyReLU)      (None, 14, 14, 256)  0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 14, 14, 256)  589824      leaky_re_lu_23[0][0]             
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 14, 14, 256)  1024        conv2d_19[0][0]                  
__________________________________________________________________________________________________
add_10 (Add)                    (None, 14, 14, 256)  0           batch_normalization_24[0][0]     
                                                                 leaky_re_lu_22[0][0]             
__________________________________________________________________________________________________
leaky_re_lu_24 (LeakyReLU)      (None, 14, 14, 256)  0           add_10[0][0]                     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 14, 14, 256)  65536       leaky_re_lu_24[0][0]             
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 14, 14, 256)  1024        conv2d_20[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_25 (LeakyReLU)      (None, 14, 14, 256)  0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 14, 14, 256)  589824      leaky_re_lu_25[0][0]             
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 14, 14, 256)  1024        conv2d_21[0][0]                  
__________________________________________________________________________________________________
add_11 (Add)                    (None, 14, 14, 256)  0           batch_normalization_26[0][0]     
                                                                 leaky_re_lu_24[0][0]             
__________________________________________________________________________________________________
leaky_re_lu_26 (LeakyReLU)      (None, 14, 14, 256)  0           add_11[0][0]                     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 14, 14, 256)  65536       leaky_re_lu_26[0][0]             
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 14, 14, 256)  1024        conv2d_22[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_27 (LeakyReLU)      (None, 14, 14, 256)  0           batch_normalization_27[0][0]     
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 14, 14, 256)  589824      leaky_re_lu_27[0][0]             
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 14, 14, 256)  1024        conv2d_23[0][0]                  
__________________________________________________________________________________________________
add_12 (Add)                    (None, 14, 14, 256)  0           batch_normalization_28[0][0]     
                                                                 leaky_re_lu_26[0][0]             
__________________________________________________________________________________________________
leaky_re_lu_28 (LeakyReLU)      (None, 14, 14, 256)  0           add_12[0][0]                     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 14, 14, 256)  65536       leaky_re_lu_28[0][0]             
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 14, 14, 256)  1024        conv2d_24[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_29 (LeakyReLU)      (None, 14, 14, 256)  0           batch_normalization_29[0][0]     
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 14, 14, 256)  589824      leaky_re_lu_29[0][0]             
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 14, 14, 256)  1024        conv2d_25[0][0]                  
__________________________________________________________________________________________________
add_13 (Add)                    (None, 14, 14, 256)  0           batch_normalization_30[0][0]     
                                                                 leaky_re_lu_28[0][0]             
__________________________________________________________________________________________________
leaky_re_lu_30 (LeakyReLU)      (None, 14, 14, 256)  0           add_13[0][0]                     
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 6, 6, 512)    1179648     leaky_re_lu_30[0][0]             
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 6, 6, 512)    2048        conv2d_26[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_31 (LeakyReLU)      (None, 6, 6, 512)    0           batch_normalization_31[0][0]     
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 6, 6, 512)    262144      leaky_re_lu_31[0][0]             
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 6, 6, 512)    2048        conv2d_27[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_32 (LeakyReLU)      (None, 6, 6, 512)    0           batch_normalization_32[0][0]     
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 6, 6, 512)    2359296     leaky_re_lu_32[0][0]             
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 6, 6, 512)    2048        conv2d_28[0][0]                  
__________________________________________________________________________________________________
add_14 (Add)                    (None, 6, 6, 512)    0           batch_normalization_33[0][0]     
                                                                 leaky_re_lu_31[0][0]             
__________________________________________________________________________________________________
leaky_re_lu_33 (LeakyReLU)      (None, 6, 6, 512)    0           add_14[0][0]                     
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 6, 6, 512)    262144      leaky_re_lu_33[0][0]             
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 6, 6, 512)    2048        conv2d_29[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_34 (LeakyReLU)      (None, 6, 6, 512)    0           batch_normalization_34[0][0]     
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 6, 6, 512)    2359296     leaky_re_lu_34[0][0]             
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 6, 6, 512)    2048        conv2d_30[0][0]                  
__________________________________________________________________________________________________
add_15 (Add)                    (None, 6, 6, 512)    0           batch_normalization_35[0][0]     
                                                                 leaky_re_lu_33[0][0]             
__________________________________________________________________________________________________
leaky_re_lu_35 (LeakyReLU)      (None, 6, 6, 512)    0           add_15[0][0]                     
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 6, 6, 512)    262144      leaky_re_lu_35[0][0]             
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 6, 6, 512)    2048        conv2d_31[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_36 (LeakyReLU)      (None, 6, 6, 512)    0           batch_normalization_36[0][0]     
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 6, 6, 512)    2359296     leaky_re_lu_36[0][0]             
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 6, 6, 512)    2048        conv2d_32[0][0]                  
__________________________________________________________________________________________________
add_16 (Add)                    (None, 6, 6, 512)    0           batch_normalization_37[0][0]     
                                                                 leaky_re_lu_35[0][0]             
__________________________________________________________________________________________________
leaky_re_lu_37 (LeakyReLU)      (None, 6, 6, 512)    0           add_16[0][0]                     
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 2, 2, 1024)   4718592     leaky_re_lu_37[0][0]             
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 2, 2, 1024)   4096        conv2d_33[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_38 (LeakyReLU)      (None, 2, 2, 1024)   0           batch_normalization_38[0][0]     
__________________________________________________________________________________________________
FC (Flatten)                    (None, 4096)         0           leaky_re_lu_38[0][0]             
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 2)            8194        FC[0][0]                         
==================================================================================================
Total params: 18,897,602
Trainable params: 18,878,402
Non-trainable params: 19,200
__________________________________________________________________________________________________
Train on 108 samples, validate on 6 samples
Epoch 1/40

 32/108 [=======>......................] - ETA: 33s - loss: 0.8569 - acc: 0.5625
 64/108 [================>.............] - ETA: 16s - loss: 2.1909 - acc: 0.5781
 96/108 [=========================>....] - ETA: 4s - loss: 2.2027 - acc: 0.5521 
108/108 [==============================] - 37s 343ms/step - loss: 2.0458 - acc: 0.5741 - val_loss: 8.0590 - val_acc: 0.5000

Epoch 00001: val_loss improved from inf to 8.05905, saving model to model/resnet34/resource/model.h5
Epoch 2/40

 32/108 [=======>......................] - ETA: 21s - loss: 1.0924 - acc: 0.5625
 64/108 [================>.............] - ETA: 12s - loss: 1.3486 - acc: 0.5469
 96/108 [=========================>....] - ETA: 3s - loss: 1.3071 - acc: 0.5312 
108/108 [==============================] - 31s 289ms/step - loss: 1.2561 - acc: 0.5278 - val_loss: 5.6192 - val_acc: 0.5000

Epoch 00002: val_loss improved from 8.05905 to 5.61918, saving model to model/resnet34/resource/model.h5
Epoch 3/40

 32/108 [=======>......................] - ETA: 21s - loss: 0.6975 - acc: 0.5312
 64/108 [================>.............] - ETA: 15s - loss: 1.2545 - acc: 0.5000
 96/108 [=========================>....] - ETA: 4s - loss: 1.2180 - acc: 0.5104 
108/108 [==============================] - 42s 389ms/step - loss: 1.1385 - acc: 0.5463 - val_loss: 2.0536 - val_acc: 0.6667

Epoch 00003: val_loss improved from 5.61918 to 2.05356, saving model to model/resnet34/resource/model.h5
Epoch 4/40

 32/108 [=======>......................] - ETA: 26s - loss: 0.6253 - acc: 0.5625
 64/108 [================>.............] - ETA: 14s - loss: 1.1037 - acc: 0.5625
 96/108 [=========================>....] - ETA: 3s - loss: 1.1364 - acc: 0.5417 
108/108 [==============================] - 34s 318ms/step - loss: 1.0616 - acc: 0.5648 - val_loss: 1.0881 - val_acc: 0.5000

Epoch 00004: val_loss improved from 2.05356 to 1.08806, saving model to model/resnet34/resource/model.h5
Epoch 5/40

 32/108 [=======>......................] - ETA: 21s - loss: 0.6452 - acc: 0.5938
 64/108 [================>.............] - ETA: 12s - loss: 1.1221 - acc: 0.5625
 96/108 [=========================>....] - ETA: 3s - loss: 1.1331 - acc: 0.5417 
108/108 [==============================] - 32s 293ms/step - loss: 1.0534 - acc: 0.5741 - val_loss: 0.6403 - val_acc: 0.5000

Epoch 00005: val_loss improved from 1.08806 to 0.64026, saving model to model/resnet34/resource/model.h5
Epoch 6/40

 32/108 [=======>......................] - ETA: 22s - loss: 0.5934 - acc: 0.6562
 64/108 [================>.............] - ETA: 12s - loss: 1.0645 - acc: 0.7188
 96/108 [=========================>....] - ETA: 3s - loss: 1.0868 - acc: 0.6458 
108/108 [==============================] - 31s 290ms/step - loss: 1.0139 - acc: 0.6667 - val_loss: 0.5763 - val_acc: 0.5000

Epoch 00006: val_loss improved from 0.64026 to 0.57632, saving model to model/resnet34/resource/model.h5
Epoch 7/40

 32/108 [=======>......................] - ETA: 25s - loss: 0.5929 - acc: 0.5625
 64/108 [================>.............] - ETA: 14s - loss: 1.0505 - acc: 0.6406
 96/108 [=========================>....] - ETA: 3s - loss: 1.0770 - acc: 0.5938 
108/108 [==============================] - 34s 316ms/step - loss: 0.9980 - acc: 0.6204 - val_loss: 0.5610 - val_acc: 0.6667

Epoch 00007: val_loss improved from 0.57632 to 0.56098, saving model to model/resnet34/resource/model.h5
Epoch 8/40

 32/108 [=======>......................] - ETA: 21s - loss: 0.5682 - acc: 0.7188
 64/108 [================>.............] - ETA: 12s - loss: 1.0322 - acc: 0.6875
 96/108 [=========================>....] - ETA: 3s - loss: 1.0669 - acc: 0.6250 
108/108 [==============================] - 31s 288ms/step - loss: 0.9871 - acc: 0.6481 - val_loss: 0.5731 - val_acc: 0.6667

Epoch 00008: val_loss did not improve from 0.56098
Epoch 9/40

 32/108 [=======>......................] - ETA: 21s - loss: 0.5390 - acc: 0.7500
 64/108 [================>.............] - ETA: 12s - loss: 1.0035 - acc: 0.7188
 96/108 [=========================>....] - ETA: 3s - loss: 1.0432 - acc: 0.6354 
108/108 [==============================] - 31s 287ms/step - loss: 0.9651 - acc: 0.6574 - val_loss: 0.5528 - val_acc: 0.6667

Epoch 00009: val_loss improved from 0.56098 to 0.55282, saving model to model/resnet34/resource/model.h5
Epoch 10/40

 32/108 [=======>......................] - ETA: 21s - loss: 0.5326 - acc: 0.7812
 64/108 [================>.............] - ETA: 12s - loss: 0.7384 - acc: 0.7344
 96/108 [=========================>....] - ETA: 3s - loss: 0.8567 - acc: 0.6667 
108/108 [==============================] - 31s 286ms/step - loss: 0.8001 - acc: 0.6852 - val_loss: 0.5651 - val_acc: 0.6667

Epoch 00010: val_loss did not improve from 0.55282
Epoch 11/40

 32/108 [=======>......................] - ETA: 21s - loss: 0.6504 - acc: 0.6875
 64/108 [================>.............] - ETA: 13s - loss: 0.6231 - acc: 0.7500
 96/108 [=========================>....] - ETA: 3s - loss: 0.6169 - acc: 0.6771 
108/108 [==============================] - 34s 313ms/step - loss: 0.5961 - acc: 0.6852 - val_loss: 0.6751 - val_acc: 0.6667

Epoch 00011: val_loss did not improve from 0.55282
Epoch 12/40

 32/108 [=======>......................] - ETA: 25s - loss: 0.5517 - acc: 0.6875
 64/108 [================>.............] - ETA: 14s - loss: 0.5214 - acc: 0.6875
 96/108 [=========================>....] - ETA: 3s - loss: 0.5765 - acc: 0.6250 
108/108 [==============================] - 34s 315ms/step - loss: 0.5570 - acc: 0.6389 - val_loss: 0.5934 - val_acc: 0.6667

Epoch 00012: val_loss did not improve from 0.55282
Epoch 13/40

 32/108 [=======>......................] - ETA: 22s - loss: 0.5587 - acc: 0.7812
 64/108 [================>.............] - ETA: 12s - loss: 0.4892 - acc: 0.7812
 96/108 [=========================>....] - ETA: 3s - loss: 0.5231 - acc: 0.7396 
108/108 [==============================] - 32s 299ms/step - loss: 0.4996 - acc: 0.7500 - val_loss: 0.5495 - val_acc: 0.6667

Epoch 00013: val_loss improved from 0.55282 to 0.54946, saving model to model/resnet34/resource/model.h5
Epoch 14/40

 32/108 [=======>......................] - ETA: 22s - loss: 0.4761 - acc: 0.7500
 64/108 [================>.............] - ETA: 13s - loss: 0.4282 - acc: 0.7812
 96/108 [=========================>....] - ETA: 3s - loss: 0.4622 - acc: 0.7604 
108/108 [==============================] - 33s 307ms/step - loss: 0.4384 - acc: 0.7685 - val_loss: 0.6565 - val_acc: 0.6667

Epoch 00014: val_loss did not improve from 0.54946
Epoch 15/40

 32/108 [=======>......................] - ETA: 23s - loss: 0.4467 - acc: 0.8125
 64/108 [================>.............] - ETA: 14s - loss: 0.3970 - acc: 0.8438
 96/108 [=========================>....] - ETA: 3s - loss: 0.4234 - acc: 0.8021 
108/108 [==============================] - 36s 329ms/step - loss: 0.4009 - acc: 0.8056 - val_loss: 0.7759 - val_acc: 0.6667

Epoch 00015: val_loss did not improve from 0.54946
Epoch 16/40

 32/108 [=======>......................] - ETA: 23s - loss: 0.3964 - acc: 0.8125
 64/108 [================>.............] - ETA: 13s - loss: 0.3338 - acc: 0.8438
 96/108 [=========================>....] - ETA: 3s - loss: 0.3691 - acc: 0.8333 
108/108 [==============================] - 33s 309ms/step - loss: 0.3480 - acc: 0.8426 - val_loss: 0.9183 - val_acc: 0.6667

Epoch 00016: val_loss did not improve from 0.54946
Epoch 17/40

 32/108 [=======>......................] - ETA: 23s - loss: 0.3862 - acc: 0.7812
 64/108 [================>.............] - ETA: 13s - loss: 0.3095 - acc: 0.8281
 96/108 [=========================>....] - ETA: 3s - loss: 0.3446 - acc: 0.8125 
108/108 [==============================] - 33s 308ms/step - loss: 0.3247 - acc: 0.8241 - val_loss: 1.0331 - val_acc: 0.8333

Epoch 00017: val_loss did not improve from 0.54946
Epoch 18/40

 32/108 [=======>......................] - ETA: 22s - loss: 0.3477 - acc: 0.8438
 64/108 [================>.............] - ETA: 13s - loss: 0.2789 - acc: 0.8594
 96/108 [=========================>....] - ETA: 3s - loss: 0.3149 - acc: 0.8438 
108/108 [==============================] - 35s 320ms/step - loss: 0.2981 - acc: 0.8519 - val_loss: 1.0555 - val_acc: 0.8333

Epoch 00018: val_loss did not improve from 0.54946
Epoch 19/40

 32/108 [=======>......................] - ETA: 24s - loss: 0.3315 - acc: 0.8750
 64/108 [================>.............] - ETA: 13s - loss: 0.2585 - acc: 0.8906
 96/108 [=========================>....] - ETA: 3s - loss: 0.2934 - acc: 0.8750 
108/108 [==============================] - 35s 325ms/step - loss: 0.2773 - acc: 0.8796 - val_loss: 1.2782 - val_acc: 0.8333

Epoch 00019: val_loss did not improve from 0.54946
Epoch 20/40

 32/108 [=======>......................] - ETA: 21s - loss: 0.2566 - acc: 0.9062
 64/108 [================>.............] - ETA: 12s - loss: 0.2161 - acc: 0.9219
 96/108 [=========================>....] - ETA: 3s - loss: 0.2758 - acc: 0.8750 
108/108 [==============================] - 31s 288ms/step - loss: 0.2602 - acc: 0.8796 - val_loss: 1.2273 - val_acc: 0.8333

Epoch 00020: val_loss did not improve from 0.54946
Epoch 21/40

 32/108 [=======>......................] - ETA: 22s - loss: 0.2324 - acc: 0.9375
 64/108 [================>.............] - ETA: 12s - loss: 0.2133 - acc: 0.9062
 96/108 [=========================>....] - ETA: 3s - loss: 0.2743 - acc: 0.8646 
108/108 [==============================] - 31s 288ms/step - loss: 0.2605 - acc: 0.8704 - val_loss: 0.5929 - val_acc: 0.8333

Epoch 00021: val_loss did not improve from 0.54946
Epoch 22/40

 32/108 [=======>......................] - ETA: 26s - loss: 0.3192 - acc: 0.8438
 64/108 [================>.............] - ETA: 14s - loss: 0.2471 - acc: 0.9062
 96/108 [=========================>....] - ETA: 3s - loss: 0.2878 - acc: 0.8854 
108/108 [==============================] - 36s 331ms/step - loss: 0.2674 - acc: 0.8981 - val_loss: 1.5515 - val_acc: 0.8333

Epoch 00022: val_loss did not improve from 0.54946
Epoch 23/40

 32/108 [=======>......................] - ETA: 23s - loss: 0.1909 - acc: 0.9375
 64/108 [================>.............] - ETA: 13s - loss: 0.1803 - acc: 0.9375
 96/108 [=========================>....] - ETA: 3s - loss: 0.2391 - acc: 0.9167 
108/108 [==============================] - 33s 307ms/step - loss: 0.2231 - acc: 0.9167 - val_loss: 1.3183 - val_acc: 0.8333

Epoch 00023: val_loss did not improve from 0.54946
Epoch 24/40

 32/108 [=======>......................] - ETA: 21s - loss: 0.1846 - acc: 0.9375
 64/108 [================>.............] - ETA: 13s - loss: 0.1685 - acc: 0.9531
 96/108 [=========================>....] - ETA: 3s - loss: 0.2238 - acc: 0.9271 
108/108 [==============================] - 34s 319ms/step - loss: 0.2050 - acc: 0.9352 - val_loss: 1.2278 - val_acc: 0.8333

Epoch 00024: val_loss did not improve from 0.54946
Epoch 25/40

 32/108 [=======>......................] - ETA: 22s - loss: 0.1379 - acc: 0.9688
 64/108 [================>.............] - ETA: 13s - loss: 0.1305 - acc: 0.9688
 96/108 [=========================>....] - ETA: 3s - loss: 0.2034 - acc: 0.9375 
108/108 [==============================] - 33s 306ms/step - loss: 0.1834 - acc: 0.9444 - val_loss: 1.3290 - val_acc: 0.8333

Epoch 00025: val_loss did not improve from 0.54946
Epoch 26/40

 32/108 [=======>......................] - ETA: 22s - loss: 0.1077 - acc: 1.0000
 64/108 [================>.............] - ETA: 13s - loss: 0.1122 - acc: 0.9844
 96/108 [=========================>....] - ETA: 3s - loss: 0.2039 - acc: 0.9375 
108/108 [==============================] - 33s 306ms/step - loss: 0.1827 - acc: 0.9444 - val_loss: 1.6966 - val_acc: 0.8333

Epoch 00026: val_loss did not improve from 0.54946
Epoch 27/40

 32/108 [=======>......................] - ETA: 23s - loss: 0.0914 - acc: 1.0000
 64/108 [================>.............] - ETA: 13s - loss: 0.0985 - acc: 0.9844
 96/108 [=========================>....] - ETA: 3s - loss: 0.1724 - acc: 0.9479 
108/108 [==============================] - 32s 301ms/step - loss: 0.1550 - acc: 0.9537 - val_loss: 1.3829 - val_acc: 0.8333

Epoch 00027: val_loss did not improve from 0.54946
Epoch 28/40

 32/108 [=======>......................] - ETA: 22s - loss: 0.1003 - acc: 1.0000
 64/108 [================>.............] - ETA: 13s - loss: 0.0785 - acc: 1.0000
 96/108 [=========================>....] - ETA: 3s - loss: 0.1522 - acc: 0.9583 
108/108 [==============================] - 33s 302ms/step - loss: 0.1371 - acc: 0.9630 - val_loss: 1.4694 - val_acc: 0.6667

Epoch 00028: val_loss did not improve from 0.54946
Epoch 29/40

 32/108 [=======>......................] - ETA: 21s - loss: 0.1216 - acc: 0.9688
 64/108 [================>.............] - ETA: 12s - loss: 0.0718 - acc: 0.9844
 96/108 [=========================>....] - ETA: 3s - loss: 0.1571 - acc: 0.9479 
108/108 [==============================] - 32s 301ms/step - loss: 0.1404 - acc: 0.9537 - val_loss: 1.7653 - val_acc: 0.6667

Epoch 00029: val_loss did not improve from 0.54946
Epoch 30/40

 32/108 [=======>......................] - ETA: 22s - loss: 0.0576 - acc: 1.0000
 64/108 [================>.............] - ETA: 13s - loss: 0.0379 - acc: 1.0000
 96/108 [=========================>....] - ETA: 3s - loss: 0.1278 - acc: 0.9583 
108/108 [==============================] - 34s 316ms/step - loss: 0.1145 - acc: 0.9630 - val_loss: 1.5190 - val_acc: 0.6667

Epoch 00030: val_loss did not improve from 0.54946
Epoch 31/40

 32/108 [=======>......................] - ETA: 21s - loss: 0.0480 - acc: 1.0000
 64/108 [================>.............] - ETA: 12s - loss: 0.0317 - acc: 1.0000
 96/108 [=========================>....] - ETA: 3s - loss: 0.1280 - acc: 0.9583 
108/108 [==============================] - 32s 298ms/step - loss: 0.1147 - acc: 0.9630 - val_loss: 1.6866 - val_acc: 0.5000

Epoch 00031: val_loss did not improve from 0.54946
Epoch 32/40

 32/108 [=======>......................] - ETA: 24s - loss: 0.0380 - acc: 1.0000
 64/108 [================>.............] - ETA: 14s - loss: 0.0273 - acc: 1.0000
 96/108 [=========================>....] - ETA: 3s - loss: 0.1047 - acc: 0.9688 
108/108 [==============================] - 36s 334ms/step - loss: 0.0939 - acc: 0.9722 - val_loss: 2.1055 - val_acc: 0.6667

Epoch 00032: val_loss did not improve from 0.54946
Epoch 33/40

 32/108 [=======>......................] - ETA: 25s - loss: 0.1417 - acc: 0.9375
 64/108 [================>.............] - ETA: 15s - loss: 0.0793 - acc: 0.9688
 96/108 [=========================>....] - ETA: 4s - loss: 0.1415 - acc: 0.9479 
108/108 [==============================] - 38s 355ms/step - loss: 0.1270 - acc: 0.9537 - val_loss: 1.9317 - val_acc: 0.6667

Epoch 00033: val_loss did not improve from 0.54946
Epoch 34/40

 32/108 [=======>......................] - ETA: 24s - loss: 0.0817 - acc: 1.0000
 64/108 [================>.............] - ETA: 14s - loss: 0.0711 - acc: 0.9844
 96/108 [=========================>....] - ETA: 3s - loss: 0.1505 - acc: 0.9375 
108/108 [==============================] - 34s 316ms/step - loss: 0.1347 - acc: 0.9444 - val_loss: 1.3333 - val_acc: 0.6667

Epoch 00034: val_loss did not improve from 0.54946
Epoch 35/40

 32/108 [=======>......................] - ETA: 21s - loss: 0.0940 - acc: 1.0000
 64/108 [================>.............] - ETA: 12s - loss: 0.0846 - acc: 0.9844
 96/108 [=========================>....] - ETA: 3s - loss: 0.1191 - acc: 0.9688 
108/108 [==============================] - 33s 304ms/step - loss: 0.1071 - acc: 0.9722 - val_loss: 1.2586 - val_acc: 0.8333

Epoch 00035: val_loss did not improve from 0.54946
Epoch 36/40

 32/108 [=======>......................] - ETA: 25s - loss: 0.0648 - acc: 1.0000
 64/108 [================>.............] - ETA: 14s - loss: 0.0645 - acc: 0.9844
 96/108 [=========================>....] - ETA: 3s - loss: 0.1165 - acc: 0.9688 
108/108 [==============================] - 34s 319ms/step - loss: 0.1040 - acc: 0.9722 - val_loss: 1.6824 - val_acc: 0.6667

Epoch 00036: val_loss did not improve from 0.54946
Epoch 37/40

 32/108 [=======>......................] - ETA: 21s - loss: 0.0253 - acc: 1.0000
 64/108 [================>.............] - ETA: 14s - loss: 0.0196 - acc: 1.0000
 96/108 [=========================>....] - ETA: 3s - loss: 0.0815 - acc: 0.9792 
108/108 [==============================] - 35s 328ms/step - loss: 0.0745 - acc: 0.9815 - val_loss: 2.3798 - val_acc: 0.6667

Epoch 00037: val_loss did not improve from 0.54946
Epoch 38/40

 32/108 [=======>......................] - ETA: 23s - loss: 0.0373 - acc: 1.0000
 64/108 [================>.............] - ETA: 13s - loss: 0.0241 - acc: 1.0000
 96/108 [=========================>....] - ETA: 3s - loss: 0.0916 - acc: 0.9688 
108/108 [==============================] - 33s 309ms/step - loss: 0.0821 - acc: 0.9722 - val_loss: 2.4467 - val_acc: 0.6667

Epoch 00038: val_loss did not improve from 0.54946
Epoch 39/40

 32/108 [=======>......................] - ETA: 21s - loss: 0.0351 - acc: 1.0000
 64/108 [================>.............] - ETA: 12s - loss: 0.0297 - acc: 1.0000
 96/108 [=========================>....] - ETA: 3s - loss: 0.0860 - acc: 0.9792 
108/108 [==============================] - 32s 292ms/step - loss: 0.0768 - acc: 0.9815 - val_loss: 1.9407 - val_acc: 0.6667

Epoch 00039: val_loss did not improve from 0.54946
Epoch 40/40

 32/108 [=======>......................] - ETA: 25s - loss: 0.0546 - acc: 0.9688
 64/108 [================>.............] - ETA: 13s - loss: 0.0319 - acc: 0.9844
 96/108 [=========================>....] - ETA: 3s - loss: 0.0816 - acc: 0.9688 
108/108 [==============================] - 34s 318ms/step - loss: 0.0731 - acc: 0.9722 - val_loss: 2.7070 - val_acc: 0.6667

Epoch 00040: val_loss did not improve from 0.54946
Loaded model from disk

6/6 [==============================] - 2s 299ms/step
acc: 66.67%

6/6 [==============================] - 0s 80ms/step
Loss = 0.5872207283973694
Test Accuracy = 0.6666666865348816
